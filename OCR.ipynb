{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python378jvsc74a57bd08182dd0ad4173cdddc5298b982923877be00ee4d2f569af5d6459bf4e939e93c",
   "display_name": "Python 3.7.8 64-bit ('aiffel': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<br>\n",
    "\n",
    "# OCR\n",
    "---\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "## Overall structure of OCR <br><br>\n",
    "\n",
    "\n",
    "### OCR = Text Detection + Text Recognition <br><br>\n",
    "\n",
    "\n",
    "- __Text Detection__ 은 Segmentation 기반의 CRAFT 를 활용한 keras-ocr 라이브러리를 활용 <br><br>\n",
    "\n",
    "    __CRAFT__ <br>\n",
    "    개별 문자(character)에 대한 annotation 부족을 극복하기 위해 <br>\n",
    "    character-level annotation 과 이미지 내에서 찾고자 하는 문자(character)의 annotation 을 모두 사용하여 <br>\n",
    "    '문자들(characters) 사이의 affinity (관련성) 를 추정하는 방식을 사용. <br>\n",
    "    \\- 논문 abstract 일부 <br><br>\n",
    "\n",
    "    참고. <br>\n",
    "    [keras-ocr official github](https://github.com/faustomorales/keras-ocr) <br>\n",
    "    [\\[paper\\] CRAFT (Character-Region Awareness For Text detection)](https://arxiv.org/pdf/1904.01941.pdf) <br>\n",
    "    [CRAFT Pytorch implementation](https://github.com/clovaai/CRAFT-pytorch) <br>\n",
    "    [CRAFT Keras github](https://github.com/notAI-tech/keras-craft) <br><br>\n",
    "\n",
    "\n",
    "- __Text Recognition__ 은 라이브러리 사용하지 않고 직접 구현 <br><br>\n",
    "\n",
    "    CTC 로 학습된 CRNN 이 사용됨 <br>\n",
    "    CRNN : CNN + RNN + FCN 구조 네트워크 <br><br>\n",
    "\n",
    "    __CRNN__ <br>\n",
    "    CNN 과 RNN 의 아이디어를 결합한 Text Recognition 의 초기 모델 (2015 년 등장). <br><br>\n",
    "\n",
    "    CRNN 이후 <br>\n",
    "    다양한 모델들이 Text Recognition task 의 새로운 기법을 제시하며 성능 향상 <br><br>\n",
    "\n",
    "    참고. <br>\n",
    "    [\\[paper\\] What is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis](https://arxiv.org/pdf/1904.01906.pdf) <br>\n",
    "    위 논문에 의하면, CRNN 보다 높은 성능을 낸 모델이 갖는 두 가지 다른 점은, <br>\n",
    "    입력이미지 변환 단계에서는 모델의 앞에서 글자를 Thin plate spline Transformation을 해주는 TPS 모듈이 붙는 점과 <br>\n",
    "    마지막 Text 출력 단계에서는 Bidirectional LSTM 뒤로 Attention decoder가 붙는 점.\n",
    "    \n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "## Dataset for OCR <br><br>\n",
    "\n",
    "OCR 데이터셋에 필요한 많은 텍스트 정보를 사람이 만들어 내는 수고로움을 해결하기 위해, <br>\n",
    "컴퓨터로 원하는 언어, 폰트, 배치, 크기로 문자 이미지를 대량으로 만들어 사용합니다. <br><br>\n",
    "\n",
    "\n",
    "### MJ Synth 데이터셋 <br><br>\n",
    "\n",
    "이런식으로 만들어진 대량의 Text Dataset 은 Text Recognition model 의 정량적 평가를 위해 많이 사용됩니다. <br>\n",
    "주로 아래의 두 가지가 많이 사용됩니다. <br><br>\n",
    "\n",
    "1. [MJ Synth](http://www.robots.ox.ac.uk/~vgg/data/text/) <br>\n",
    "2. [SynthText](http://www.robots.ox.ac.uk/~vgg/data/scenetext/) <br><br>\n",
    "\n",
    "    링크. <br>\n",
    "    Naver Clova 의 논문 저자 제공 MJ Synth 데이터셋 다운로드 링크 : [Dropbox-data_lmdb_release](https://www.dropbox.com/sh/i39abvnefllx2si/AAAbAYRvxzRp3cIE5HzqUw3ra?dl=0) <br><br>\n",
    "\n",
    "    커널. <br>\n",
    "    위 링크의 다운로드를 커널을 통해 실행합니다.\n",
    "    ```\n",
    "    $ mkdir -p ~/aiffel/ocr\n",
    "    $ cd ~/aiffel/ocr\n",
    "    $ wget https://www.dropbox.com/sh/i39abvnefllx2si/AABX4yjNn2iLeKZh1OAwJUffa/data_lmdb_release.zip\n",
    "    $ unzip data_lmdb_release.zip\n",
    "    $ mv data_lmdb_release/training/MJ .  # data_lmdb_release/training/MJ 아래의 데이터만 ~/aiffel/ocr 아래로 가져옵니다.\n",
    "    # 이후 불필요한 data_lmdb_release.zip 및 data_lmdb_release 하위의 남은 데이터는 삭제하셔도 무방합니다.\n",
    "    ```\n",
    "    <br>\n",
    "    \n",
    "\n",
    "### LMDB (Lightning Memory-Mapped Database) <br><br>\n",
    "\n",
    "데이터셋은 lmdb 포맷(.mdb) 파일로 이루어져 있습니다. <br>\n",
    "LMFB 는 Symas 에서 만든 Lightning Memory-Mapped Database 입니다. <br><br>\n",
    "\n",
    "- lmdb 데이터셋을 다루기 위해 파이썬을 위한 lmdb 모듈을 받아야 합니다. <br><br>\n",
    "\n",
    "    ```\n",
    "    $ pip install lmdb\n",
    "    # or\n",
    "    $ conda install -c conda-forge python-lmdb\n",
    "    ```\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/ssac29/aiffel/ocr\n"
     ]
    }
   ],
   "source": [
    "# 기본 경로 설정 \n",
    "\n",
    "import os\n",
    "\n",
    "path = os.path.join(os.getenv('HOME'),'aiffel/ocr')\n",
    "os.chdir(path)\n",
    "\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "\n",
    "import re\n",
    "import six\n",
    "import math\n",
    "import lmdb\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/ssac29/aiffel/ocr/MJ/MJ_train\n"
     ]
    }
   ],
   "source": [
    "# lmdb 라이브러리로 MJ dataset 불러오기\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "HOME_DIR = os.getenv('HOME')+'/aiffel/ocr'\n",
    "\n",
    "# 로컬 사용자\n",
    "TRAIN_DATA_PATH = HOME_DIR+'/MJ/MJ_train'\n",
    "VALID_DATA_PATH = HOME_DIR+'/MJ/MJ_valid'\n",
    "TEST_DATA_PATH = HOME_DIR+'/MJ/MJ_test'\n",
    "\n",
    "# 클라우드 사용자는 아래 주석을 사용해 주세요.\n",
    "# TRAIN_DATA_PATH = HOME_DIR+'/data/MJ/MJ_train'\n",
    "# VALID_DATA_PATH = HOME_DIR+'/data/MJ/MJ_valid'\n",
    "# TEST_DATA_PATH = HOME_DIR+'/data/MJ/MJ_test'\n",
    "print(TRAIN_DATA_PATH)"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "## Text Recognition 모델 구현 <br><br>\n",
    "\n",
    "논문에서 소개된 CRNN 구조를 활용하여 Text Recognition 모델 구현 <br>\n",
    "\n",
    "- __CRNN__ <br>\n",
    "    CNN + RNN + FCN 구조 네트워크 <br><br>\n",
    "\n",
    "    참고. <br>\n",
    "    [\\[paper\\] An End-to-End Trainable Neural Network for Image-based SequenceRecognition and Its Application to Scene Text Recognition](https://arxiv.org/pdf/1507.05717.pdf)\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### CRNN 네트워크의 구조 <br><br>\n",
    "\n",
    "Convolution layer 를 통해 feature 추출 <br>\n",
    "Recurrent layer 를 통해 추출된 feature 의 전체적인 context 를 파악. 다양한 output 크기에 대응 <br>\n",
    "Transcription layer 를 통해 step 마다 어떤 character 의 확률이 높은지 예측. <br>\n",
    "( Transcription layer 는 Fully Connected Layer ) <br><br>\n",
    "\n",
    "\n",
    "Type | Configuragions\n",
    ":---: | :---:\n",
    "Transcription | -\n",
    "Bidirectional-LSTM | # hidden units : 256\n",
    "Bidirectional-LSTM | # hidden units : 256\n",
    "Map-to-Sequence | -\n",
    "Convolution | # maps : 512, k : 2x2, s : 1, p : 0\n",
    "MaxPooling | Window : 1x2, s : 2\n",
    "BatchNormalization | -\n",
    "Convolution | # maps : 512, k : 3x3, s : 1, p : 1\n",
    "BatchNormalization | -\n",
    "Convolution | # maps : 512, k : 3x3, s : 1, p : 1\n",
    "MaxPooling | Window : 1x2, s : 2\n",
    "Convolution | # maps : 256, k : 3x3, s : 1, p : 1\n",
    "Convolution | # maps : 256, k : 3x3, s : 1, p : 1\n",
    "MaxPooling | Window : 2x2, s : 2\n",
    "Convolution | # maps : 128, k : 3x3, s : 1, p : 1\n",
    "MaxPooling | Window : 2x2, s : 2\n",
    "Convolution | # maps : 64, k : 3x3, s : 1, p : 1\n",
    "Input | W x 32 gray-scale image\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The total number of characters is 36\n"
     ]
    }
   ],
   "source": [
    "# Text Recognition 할 타겟 지정\n",
    "# 여기서는 0~9 까지의 숫자 character 10가지와 A~Z 까이의 알파벳 대문자 character 26 가지로 지정\n",
    "\n",
    "NUMBERS = \"0123456789\"\n",
    "ENG_CHAR_UPPER = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "TARGET_CHARACTERS = ENG_CHAR_UPPER + NUMBERS\n",
    "\n",
    "print(f\"The total number of characters is {len(TARGET_CHARACTERS)}\")"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### Input Image : 모델 학습용 input dataset 생성 <br><br>\n",
    "\n",
    "\n",
    "Text Recognition 모델을 학습하기 위해 <br>\n",
    "모델의 입력값으로 주어지는 input text image dataset 을 생성 <br><br>\n",
    "\n",
    "\n",
    "#### MJDatasetSequence 클래스 작성 <br><br>\n",
    "\n",
    "\n",
    "lmdb 를 활용하여 케라스 모델을 학습하기 위한 MJ Synth 데이터셋을 불러오는 파이썬 클래스 구현 <br><br>\n",
    "\n",
    "\n",
    "- 구현할 인자 <br>\n",
    "\n",
    "    - ```dataet_path``` : 읽어올 데이터셋 경로 <br>\n",
    "    - ```label_converter``` : 문자를 미리 정의된 index 로 변환해주는 converter <br>\n",
    "    - ```batch_size``` : 학습용 데이터 배치 사이즈 <br>\n",
    "    - ... <br>\n",
    "    - 그 외에 필터링을 위한 최대 글자 수, 학습 대상이 될 character 한정 등 ... <br><br>\n",
    "\n",
    "- 구현할 메소드 <br>\n",
    "\n",
    "    - ```_get_img_label()``` : 이미지 데이터를 ```img, label``` 쌍으로 가져오는 메소드 <br>\n",
    "        ( 이 메소드 안에서 다양한 사이즈의 이미지를 모두 H:32 x W:최대100 으로 맞추도록 가공 ) <br>\n",
    "    - ```__getitem__()``` : 케라스의 ```model.fit()``` 에서 호출되어 배치 단위만큼 데이터셋을 가져와 리턴 <br>\n",
    "    - ...\n",
    "    \n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "original image width:72, height:31\ntarget_img_size:(74, 32)\ndisplay img shape:(74, 32, 3)\nlabel:Lube\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=74x32 at 0x7FABD1D5A290>",
      "image/png": "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\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "original image width:82, height:31\ntarget_img_size:(84, 32)\ndisplay img shape:(84, 32, 3)\nlabel:Spencerian\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=84x32 at 0x7FABD1D5A610>",
      "image/png": "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\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "original image width:115, height:31\ntarget_img_size:(100, 32)\ndisplay img shape:(100, 32, 3)\nlabel:accommodatingly\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=100x32 at 0x7FABD1D5A2D0>",
      "image/png": "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\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "original image width:140, height:31\ntarget_img_size:(100, 32)\ndisplay img shape:(100, 32, 3)\nlabel:CARPENTER\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=100x32 at 0x7FABD1D5A3D0>",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# 원본 데이터셋 안에 있는 이미지 확인 (shape, label 등 ...)\n",
    "# lmdb 를 통해 training dataset 이미지 4 개만 확인\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "env = lmdb.open(TRAIN_DATA_PATH, max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "with env.begin(write=False) as txn:\n",
    "    for index in range(1, 5):\n",
    "        label_key = 'label-%09d'.encode() % index\n",
    "        label = txn.get(label_key).decode('utf-8')\n",
    "        img_key = 'image-%09d'.encode() % index\n",
    "        imgbuf = txn.get(img_key)\n",
    "        buf = six.BytesIO()\n",
    "        buf.write(imgbuf)\n",
    "        buf.seek(0)\n",
    "        try:\n",
    "            img = Image.open(buf).convert('RGB')\n",
    "\n",
    "        except IOError:\n",
    "            img = Image.new('RGB', (100, 32))\n",
    "            label = '-'\n",
    "        width, height = img.size\n",
    "        print('original image width:{}, height:{}'.format(width, height))\n",
    "        \n",
    "        target_width = min(int(width*32/height), 100)\n",
    "        target_img_size = (target_width,32 )\n",
    "        \n",
    "        print('target_img_size:{}'.format(target_img_size))\n",
    "        \n",
    "        img = np.array(img.resize(target_img_size)).transpose(1,0,2)\n",
    "       \n",
    "        print('display img shape:{}'.format(img.shape))\n",
    "        print('label:{}'.format(label))\n",
    "        display(Image.fromarray(img.transpose(1,0,2).astype(np.uint8)))"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "대부분의 이미지의 height는 31, 최대 32 <br> \n",
    "width는 문자열 길이에 따라 다양\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Recognition model 학습을 위한 Input dataset 을 생성하는 파이썬 클래스 생성\n",
    "# MJ Synth 데이터셋을 lmdb 라이브러리로 불러와 구현\n",
    "# 케라스 모델 학습에 필요한 여러 인자와 메소드를 구현\n",
    "\n",
    "class MJDatasetSequence(Sequence):\n",
    "    def __init__(self, \n",
    "                      dataset_path,\n",
    "                      label_converter,\n",
    "                      batch_size=1,\n",
    "                      img_size=(100,32),\n",
    "                      max_text_len=22,\n",
    "                      is_train=False,\n",
    "                      character=''\n",
    "                ):\n",
    "        \n",
    "        self.label_converter = label_converter\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.max_text_len = max_text_len\n",
    "        self.character = character\n",
    "        self.is_train = is_train\n",
    "        self.divide_length = 100\n",
    "\n",
    "        self.env = lmdb.open(dataset_path, max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            num_samples = int(txn.get('num-samples'.encode()))\n",
    "            self.num_samples = int(num_samples)\n",
    "            self.index_list = [index + 1 for index in range(self.num_samples)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return math.ceil(self.num_samples/self.batch_size/self.divide_length)\n",
    "        return math.ceil(self.num_samples/self.batch_size/self.divide_length)\n",
    "    \n",
    "    # index에 해당하는 image와 label을 가져오는 메소드\n",
    "    def _get_img_label(self, index):\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            label_key = 'label-%09d'.encode() % index\n",
    "            label = txn.get(label_key).decode('utf-8')\n",
    "            img_key = 'image-%09d'.encode() % index\n",
    "            imgbuf = txn.get(img_key)\n",
    "\n",
    "            buf = six.BytesIO()\n",
    "            buf.write(imgbuf)\n",
    "            buf.seek(0)\n",
    "            try:\n",
    "                img = Image.open(buf).convert('RGB')\n",
    "\n",
    "            except IOError:\n",
    "                img = Image.new('RGB', self.img_size)\n",
    "                label = '-'\n",
    "            width, height = img.size\n",
    "            \n",
    "            target_width = min(int(width*self.img_size[1]/height), self.img_size[0])\n",
    "            target_img_size = (target_width,self.img_size[1] )\n",
    "            img = np.array(img.resize(target_img_size)).transpose(1,0,2)\n",
    "            label = label.upper()[:self.max_text_len]\n",
    "            out_of_char = f'[^{self.character}]'\n",
    "            label = re.sub(out_of_char, '', label)\n",
    "\n",
    "        return (img, label)\n",
    "    \n",
    "    # idx번째 배치를 가져오는 메소드\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indicies = self.index_list[\n",
    "            idx*self.batch_size:\n",
    "            (idx+1)*self.batch_size\n",
    "        ]\n",
    "        input_images = np.zeros([self.batch_size, *self.img_size, 3])\n",
    "        labels = np.zeros([self.batch_size, self.max_text_len], dtype='int64')\n",
    "\n",
    "        input_length = np.ones([self.batch_size], dtype='int64')*self.max_text_len\n",
    "        label_length = np.ones([self.batch_size], dtype='int64')\n",
    "\n",
    "        for i, index in enumerate(batch_indicies):\n",
    "            img, label = self._get_img_label(index)\n",
    "            encoded_label = self.label_converter.encode(label)\n",
    "            width = img.shape[0]\n",
    "            input_images[i,:width,:,:] = img\n",
    "            if len(encoded_label) > self.max_text_len:\n",
    "                continue\n",
    "            labels[i,0:len(encoded_label)] = encoded_label\n",
    "            label_length[i] = len(encoded_label)\n",
    "\n",
    "        inputs = {\n",
    "            'input_image': input_images,\n",
    "            'label': labels,\n",
    "            'input_length': input_length,\n",
    "            'label_length': label_length,\n",
    "        }\n",
    "        outputs = {'ctc': np.zeros([self.batch_size, 1])}\n",
    "        return inputs, outputs\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.index_list =  [index + 1 for index in range(self.num_samples)]\n",
    "        if self.is_train :\n",
    "            np.random.shuffle(self.index_list)\n",
    "            return self.index_list"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### Encode : Text Class -> Class Index 변환 <br><br>\n",
    "\n",
    "\n",
    "Text Recognition 모델 학습을 위해 <br>\n",
    "모델의 출력값으로 주어지는 Text Class 를 Class Index 로 변환 <br><br>\n",
    "\n",
    "\n",
    "#### LabelConverter 클래스 작성 <br><br>\n",
    "\n",
    "MJ Synth 데이터셋에서 받은 text 로 된 class 를 index 형태의 class 로 encoding 해주는 클래스를 작성 <br><br>\n",
    "\n",
    "\n",
    "- ```__init__()``` : 입력받은 text 의 각 character 들이 ```self.dict``` 의 어떤 index 에 매핑 되는지 저장 <br>\n",
    "( ex. 만약 ```character='ABCD'``` 라면, ```'A'``` 의 label 은 ```1```, ```'B'``` 의 label 은 ```2``` 가 되도록 저장 ) <br>\n",
    "- ```encode()``` : 입력받은 text 의 character 를 index 로 매핑하여 저장. 공백(blank) 문자를 지정. (```'-'``` 문자를 활용, label 은 ```0```) <br>\n",
    "- ```decode()``` : 각 index 를 다시 character 로 변환 (이후 text 로 바꿔 줌)\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋의 class 를 text -> index 로 매핑해주는 인코더 클래스 작성\n",
    "\n",
    "class LabelConverter(object):\n",
    "     \"\"\" Convert between text-label and text-index \"\"\"\n",
    "\n",
    "     def __init__(self, character):\n",
    "         self.character = \"-\" + character\n",
    "         self.label_map = dict()\n",
    "         for i, char in enumerate(self.character):\n",
    "             self.label_map[char] = i\n",
    "\n",
    "     def encode(self, text):\n",
    "         encoded_label = []\n",
    "         for i, char in enumerate(text):\n",
    "             if i > 0 and char == text[i - 1]:\n",
    "                 encoded_label.append(0)    # 같은 문자 사이에 공백 문자 label을 삽입\n",
    "             encoded_label.append(self.label_map[char])\n",
    "         return np.array(encoded_label)\n",
    "\n",
    "     def decode(self, encoded_label):\n",
    "         target_characters = list(self.character)\n",
    "         decoded_label = \"\"\n",
    "         for encode in encoded_label:\n",
    "             decoded_label += self.character[encode]\n",
    "         return decoded_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\n"
     ]
    }
   ],
   "source": [
    "# 위에서 정의한 TARGET_CHARACTERS 확인\n",
    "\n",
    "print(TARGET_CHARACTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Encdoded_text:  [ 8  5 12  0 12 15]\nDecoded_text:  HEL-LO\n"
     ]
    }
   ],
   "source": [
    "# 'HELLO' 텍스트를 encode 후 decode 하여 클래스 작동 확인\n",
    "# 동일한 글자 'L'이 연속될 때, 그 사이에 공백 문자가 포함된 것을 확인할 수 있습니다.\n",
    "\n",
    "label_converter = LabelConverter(TARGET_CHARACTERS)\n",
    "\n",
    "encdoded_text = label_converter.encode('HELLO')\n",
    "print(\"Encdoded_text: \", encdoded_text)\n",
    "decoded_text = label_converter.decode(encdoded_text)\n",
    "print(\"Decoded_text: \", decoded_text)"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### CRNN 모델 구성 <br><br>\n",
    "\n",
    "위에서 모델의 입력과 출력을 준비하였습니다. <br>\n",
    "여기에 케라스에서 제공하는 CTC Loss function 을 활용하여 CRNN 모델을 구현합니다. <br>\n",
    "( CTC Loss function 에 대한 설명은 아래 참고 ) <br><br>\n",
    "\n",
    "\n",
    "CRNN 모델은 논문에 나와있는 네트워크 구조를 참고하여 작성 <br><br>\n",
    "\n",
    "Type | Configuragions\n",
    ":---: | :---:\n",
    "Transcription | -\n",
    "Bidirectional-LSTM | # hidden units : 256\n",
    "Bidirectional-LSTM | # hidden units : 256\n",
    "Map-to-Sequence | -\n",
    "Convolution | # maps : 512, k : 2x2, s : 1, p : 0\n",
    "MaxPooling | Window : 1x2, s : 2\n",
    "BatchNormalization | -\n",
    "Convolution | # maps : 512, k : 3x3, s : 1, p : 1\n",
    "BatchNormalization | -\n",
    "Convolution | # maps : 512, k : 3x3, s : 1, p : 1\n",
    "MaxPooling | Window : 1x2, s : 2\n",
    "Convolution | # maps : 256, k : 3x3, s : 1, p : 1\n",
    "Convolution | # maps : 256, k : 3x3, s : 1, p : 1\n",
    "MaxPooling | Window : 2x2, s : 2\n",
    "Convolution | # maps : 128, k : 3x3, s : 1, p : 1\n",
    "MaxPooling | Window : 2x2, s : 2\n",
    "Convolution | # maps : 64, k : 3x3, s : 1, p : 1\n",
    "Input | W x 32 gray-scale image\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "#### CTC Loss function <br><br>\n",
    "\n",
    "\n",
    "노드 12-7 참고하여 정리 <br><br>\n",
    "\n",
    "\n",
    "```K.ctc_batch_cost(y_true, y_pred, input_length, label_length)``` 의 4가지 인자 <br>\n",
    "\n",
    "- ```y_true```: tensor (samples, max_string_length) containing the truth labels.\n",
    "- ```y_pred```: tensor (samples, time_steps, num_categories) containing the prediction, or output of the softmax.\n",
    "- ```input_length tensor```: (samples, 1) containing the sequence length for each batch item in y_pred.\n",
    "- ```label_length tensor```: (samples, 1) containing the sequence length for each batch item in y_true.\n",
    "    ( samples 는 배치사이즈를 의미) <br><br>\n",
    "\n",
    "\n",
    "참고. <br>\n",
    "[Tensorflow Tutorial - ctc_batch_cost](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/backend/ctc_batch_cost)\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스에서 제공하는 K.ctc_batch_cost() 를 이용하여 CTC loss 를 계산하는 함수 작성\n",
    "\n",
    "def ctc_lambda_func(args): # CTC loss를 계산하기 위한 Lambda 함수\n",
    "    labels, y_pred, label_length, input_length = args\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRNN 모델을 생성하는 함수 작성\n",
    "# 위에서 작성한 ctc_lambda_func() 으로 CTC loss 를 적용\n",
    "\n",
    "def build_crnn_model(input_shape=(100,32,3), characters=TARGET_CHARACTERS):\n",
    "    num_chars = len(characters)+2\n",
    "    image_input = layers.Input(shape=input_shape, dtype='float32', name='input_image')\n",
    "\n",
    "    # Build CRNN model\n",
    "    conv = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(image_input)\n",
    "    conv = layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    conv = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    conv = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(1, 2))(conv)\n",
    "    conv = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.BatchNormalization()(conv)\n",
    "    conv = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.BatchNormalization()(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(1, 2))(conv)     \n",
    "    feature = layers.Conv2D(512, (2, 2), activation='relu', kernel_initializer='he_normal')(conv)\n",
    "    sequnce = layers.Reshape(target_shape=(24, 512))(feature)\n",
    "    sequnce = layers.Dense(64, activation='relu')(sequnce)\n",
    "    sequnce = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(sequnce)\n",
    "    sequnce = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(sequnce)\n",
    "    y_pred = layers.Dense(num_chars, activation='softmax', name='output')(sequnce)\n",
    "\n",
    "    labels = layers.Input(shape=[22], dtype='int64', name='label')\n",
    "    input_length = layers.Input(shape=[1], dtype='int64', name='input_length')\n",
    "    label_length = layers.Input(shape=[1], dtype='int64', name='label_length')\n",
    "    loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name=\"ctc\")(\n",
    "        [labels, y_pred, label_length, input_length]\n",
    "    )\n",
    "    model_input = [image_input, labels, input_length, label_length]\n",
    "    model = Model(\n",
    "        inputs=model_input,\n",
    "        outputs=loss_out\n",
    "    )\n",
    "    y_func = tf.keras.backend.function(image_input, [y_pred])\n",
    "    return model, y_func"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "## 모델 학습 <br><br>\n",
    "\n",
    "MJDatasetSequence 로 데이터셋을 분리하여 학습 진행 <br>\n",
    "1 Epoch 당 10 초 정도 소요\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 9316613217667384502,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 6872093184\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 4841418291300895183\n",
       " physical_device_desc: \"device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\"]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# gpu 사용 가능여부 확인\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 및 평가용 데이터셋 준비\n",
    "# MJDatasetSequence 로 데이터셋 분리\n",
    "\n",
    "train_set = MJDatasetSequence(TRAIN_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS, is_train=True)\n",
    "val_set = MJDatasetSequence(VALID_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "\n",
    "model, y_func = build_crnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_image (InputLayer)        [(None, 100, 32, 3)] 0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 100, 32, 64)  1792        input_image[0][0]                \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 50, 16, 64)   0           conv2d[0][0]                     \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 50, 16, 128)  73856       max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 25, 8, 128)   0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 25, 8, 256)   295168      max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 25, 8, 256)   590080      conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 25, 4, 256)   0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 25, 4, 512)   1180160     max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 25, 4, 512)   2048        conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 25, 4, 512)   2359808     batch_normalization[0][0]        \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 25, 4, 512)   2048        conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 25, 2, 512)   0           batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 24, 1, 512)   1049088     max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nreshape (Reshape)               (None, 24, 512)      0           conv2d_6[0][0]                   \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 24, 64)       32832       reshape[0][0]                    \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   (None, 24, 512)      657408      dense[0][0]                      \n__________________________________________________________________________________________________\nbidirectional_1 (Bidirectional) (None, 24, 512)      1574912     bidirectional[0][0]              \n__________________________________________________________________________________________________\nlabel (InputLayer)              [(None, 22)]         0                                            \n__________________________________________________________________________________________________\noutput (Dense)                  (None, 24, 38)       19494       bidirectional_1[0][0]            \n__________________________________________________________________________________________________\nlabel_length (InputLayer)       [(None, 1)]          0                                            \n__________________________________________________________________________________________________\ninput_length (InputLayer)       [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nctc (Lambda)                    (None, 1)            0           label[0][0]                      \n                                                                 output[0][0]                     \n                                                                 label_length[0][0]               \n                                                                 input_length[0][0]               \n==================================================================================================\nTotal params: 7,838,694\nTrainable params: 7,836,646\nNon-trainable params: 2,048\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 shape 확인\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 checkpoint 경로설정 및 callback 객체 생성 \n",
    "# 학습용 sgd 설정 후 모델 컴파일\n",
    "\n",
    "checkpoint_path = HOME_DIR + '/model_checkpoint_02.hdf5'\n",
    "sgd = tf.keras.optimizers.Adadelta(lr=0.1, clipnorm=5)\n",
    "\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n",
    "ckp = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_loss',\n",
    "    verbose=1, save_best_only=True, save_weights_only=True\n",
    ")\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 2327783499476376367,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 6872093184\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 3074330913118165591\n",
       " physical_device_desc: \"device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\"]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NotFoundError",
     "evalue": "2 root error(s) found.\n  (0) Not found:  No algorithm worked!\n\t [[node model/conv2d/Relu (defined at <ipython-input-20-22c80a7be625>:8) ]]\n  (1) Not found:  No algorithm worked!\n\t [[node model/conv2d/Relu (defined at <ipython-input-20-22c80a7be625>:8) ]]\n\t [[model/ctc/Log/_76]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_13198]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-22c80a7be625>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         callbacks=[ckp])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: 2 root error(s) found.\n  (0) Not found:  No algorithm worked!\n\t [[node model/conv2d/Relu (defined at <ipython-input-20-22c80a7be625>:8) ]]\n  (1) Not found:  No algorithm worked!\n\t [[node model/conv2d/Relu (defined at <ipython-input-20-22c80a7be625>:8) ]]\n\t [[model/ctc/Log/_76]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_13198]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "\n",
    "model.fit(train_set,\n",
    "        steps_per_epoch=len(val_set),\n",
    "        epochs=100,\n",
    "        validation_data=val_set,\n",
    "        validation_steps=len(val_set),\n",
    "        callbacks=[ckp])"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "## 모델 학습 결과 확인 <br><br>\n",
    "\n",
    "테스트 데이터셋으로 학습된 모델을 통해 inference 진행 후 눈으로 확인\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument:  Labels length is zero in batch 4\n\t [[node model_2/ctc/CTCLoss (defined at <ipython-input-17-53235e0078ef>:6) ]]\n  (1) Invalid argument:  Labels length is zero in batch 4\n\t [[node model_2/ctc/CTCLoss (defined at <ipython-input-17-53235e0078ef>:6) ]]\n\t [[gradient_tape/model_2/ctc/Shape/_90]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_26395]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-12b6113a42fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         callbacks=[ckp])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  Labels length is zero in batch 4\n\t [[node model_2/ctc/CTCLoss (defined at <ipython-input-17-53235e0078ef>:6) ]]\n  (1) Invalid argument:  Labels length is zero in batch 4\n\t [[node model_2/ctc/CTCLoss (defined at <ipython-input-17-53235e0078ef>:6) ]]\n\t [[gradient_tape/model_2/ctc/Shape/_90]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_26395]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델 inference 및 확인\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "test_set = MJDatasetSequence(TEST_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS)\n",
    "\n",
    "model, y_func = build_crnn_model()\n",
    "\n",
    "model.load_weights(checkpoint_path)\n",
    "input_data = model.get_layer('input_image').output\n",
    "y_pred = model.get_layer('output').output\n",
    "model_pred = Model(inputs=input_data, outputs=y_pred)\n",
    "\n",
    "def decode_predict_ctc(out, chars = TARGET_CHARACTERS, top_paths = 1):\n",
    "    results = []\n",
    "    beam_width = 5\n",
    "    if beam_width < top_paths:\n",
    "        beam_width = top_paths\n",
    "    for i in range(top_paths):\n",
    "        indexes = K.get_value(\n",
    "            K.ctc_decode(\n",
    "                out, input_length = np.ones(out.shape[0]) * out.shape[1],\n",
    "                greedy =False , beam_width = beam_width, top_paths = top_paths\n",
    "            )[0][i]\n",
    "        )[0]\n",
    "        text = \"\"\n",
    "        for index in indexes:\n",
    "            text += chars[index]\n",
    "        results.append(text)\n",
    "    return results\n",
    "\n",
    "def check_inference(model, dataset, index = 5):\n",
    "    for i in range(index):\n",
    "        inputs, outputs = dataset[i]\n",
    "        img = dataset[i][0]['input_image'][0:1,:,:,:]\n",
    "        output = model_pred.predict(img)\n",
    "        result = decode_predict_ctc(output, chars=\"-\"+TARGET_CHARACTERS)[0].replace('-','')\n",
    "        print(\"Result: \\t\", result)\n",
    "        display(Image.fromarray(img[0].transpose(1,0,2).astype(np.uint8)))\n",
    "\n",
    "check_inference(model, test_set, index=10)"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "## 프로젝트 End-to-End OCR <br><br>\n",
    "\n",
    "\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 임포트\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Detection 함수 작성\n",
    "# 이미지 경로를 받아 해당 이미지 내의 문자를 찾아내는 함수를 keras-ocr의 Detector를 이용해서 작성\n",
    "\n",
    "def detect_text(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    line_img = img.copy()\n",
    "    input_img = img[tf.newaxis,...]\n",
    "    result=detector.detect(input_img)\n",
    "    ocr_result = result[0]\n",
    "    cropped_imgs = []\n",
    "    for text_result in ocr_result:\n",
    "        for i in range(4):\n",
    "            if i==3:\n",
    "                line_img = cv2.line(line_img,tuple(text_result[3]),tuple(text_result[0]),(0,0,255),10)\n",
    "            else:\n",
    "                line_img = cv2.line(line_img,tuple(text_result[i]),tuple(text_result[i+1]),(0,0,255),10)\n",
    "        x_min = text_result[:,0].min() - 5\n",
    "        x_max = text_result[:,0].max() + 5\n",
    "        y_min = text_result[:,1].min() - 5\n",
    "        y_max = text_result[:,1].max() + 5\n",
    "        cropped_imgs.append(img[int(y_min):int(y_max),int(x_min):int(x_max)])\n",
    "    return line_img, cropped_imgs\n",
    "img, cropped_img = detect_text(SAMPLE_IMG_PATH)\n",
    "plt.imshow(img[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference 후 시각화\n",
    "\n",
    "img_pil, cropped_img = detect_text(SAMPLE_IMG_PATH)\n",
    "display(img_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Recognition 함수 작성\n",
    "\n",
    "def recognize_img(pil_img, input_img_size=(100,32)):\n",
    "\n",
    "    # TODO: 잘려진 단어 이미지를 인식하는 코드를 작성하세요!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확인\n",
    "\n",
    "for _img in cropped_img:\n",
    "    recognize_img(_img)"
   ]
  }
 ]
}